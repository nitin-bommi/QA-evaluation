{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('textbook.md')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 601, which is longer than the specified 500\n",
      "Created a chunk of size 1017, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/nitin/.cache/weaviate-embedded: process ID 1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-03-25T10:30:14-07:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-03-25T10:30:14-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-03-25T10:30:14-07:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-03-25T10:30:15-07:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-03-25T10:30:15-07:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-03-25T10:30:15-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_2fe83ac56c4748b8b548bb8ccf95f52a_tqLYuIwX9U8W in 15.314713ms\",\"time\":\"2024-03-25T10:30:15-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:15-07:00\",\"took\":174650}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_33bc577d3b174dbe80065f14d3cdfad1_cduBP235CK5V in 4.820189ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":102455}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_4494c035b1ea484ca703291830a88a08_V4rU9Y3IiGD8 in 2.610564ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":89367}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_794ffb00ff454d8ba61f380715703f5a_xlXVAwBgx04x in 2.70765ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":104207}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_dea9ee2a8fe244fe83a7998942748188_tZyvtZMJ41RI in 2.19951ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":85899}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_d340b64b43d74a4bb35e9aa9be5c727b_UCIZpkIUujv2 in 2.910749ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":77113}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_6ca93ac0f1944d47a17f964629f3f3c0_T6t5oYlCLk7F in 7.363962ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_3bc7b6bffb1e4e93aa37e5d33bb436fc_S7PluzI0J2Jf in 7.752659ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":2251644}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_f59d63ebee2040dd95019025fe4c4668_aBKPgoJhzZ2w in 6.542739ms\",\"time\":\"2024-03-25T10:30:16-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":5016181}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T10:30:16-07:00\",\"took\":4814396}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "  client=client,\n",
    "  documents=chunks,\n",
    "  embedding=embeddings_model,\n",
    "  by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "with open(\"questions.txt\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    answer = chain.invoke(question)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_answers = []\n",
    "\n",
    "for answer in answers:\n",
    "    words = answer.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word == 'Answer:':\n",
    "            processed_answers.append(\" \".join(words[i+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\", \"w\") as f:\n",
    "    for answer in processed_answers:\n",
    "        f.write(answer)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answer = processed_answers[0]\n",
    "\n",
    "real_answer = \"\"\"\n",
    "Autoencoders are one of the deep learning types used for unsupervised learning. There are key layers of autoencoders, which are the input layer, encoder, bottleneck hidden layer, decoder, and output.\n",
    "\n",
    "The three layers of the autoencoder are:-\n",
    "1) Encoder - Compresses the input data to an encoded representation which is typically much smaller than the input data.\n",
    "2) Latent Space Representation/ Bottleneck/ Code - Compact summary of the input containing the most important features\n",
    "3) Decoder - Decompresses the knowledge representation and reconstructs the data back from its encoded form.\n",
    "Then a loss function is used at the top to compare the input and output images.\n",
    "NOTE- It's a requirement that the dimensionality of the input and output be the same. Everything in the middle can be played with.\n",
    "\n",
    "Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\n",
    "\n",
    "1. Transformers and Big Bird (Autoencoders is one of these components in both algorithms): Text Summarizer, Text Generator\n",
    "2. Image compression\n",
    "3. Nonlinear version of PCA\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answers = []\n",
    "\n",
    "with open('textbook.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "for t in text.split('---'):\n",
    "    real_answers.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(real_answers)):\n",
    "    real_embedding = embeddings_model.embed_query(real_answers[i])\n",
    "    predicted_embedding = embeddings_model.embed_query(predicted_answer)\n",
    "    similarity = cosine_similarity([predicted_embedding], [real_embedding])[0][0]\n",
    "    similarities.append(similarity)\n",
    "\n",
    "similarities = np.asarray(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.3212592276825651\n",
      "Max similarity: 0.7797517213497003\n",
      "Min similarity: 0.09688542536650491\n",
      "Standard deviation of similarities: 0.1781878540278423\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity:', np.mean(similarities))\n",
    "print('Max similarity:', np.max(similarities))\n",
    "print('Min similarity:', np.min(similarities))\n",
    "print('Standard deviation of similarities:', np.std(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
