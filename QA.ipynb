{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('textbook.md')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 601, which is longer than the specified 500\n",
      "Created a chunk of size 1017, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "  client=client,\n",
    "  documents=chunks,\n",
    "  embedding=embeddings_model,\n",
    "  by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "with open(\"questions.txt\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    answer = chain.invoke(question)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\", \"w\") as f:\n",
    "    for answer in answers:\n",
    "        f.write(answer.replace('\\\\n', '\\n'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"---\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. \n",
      "Use the following pieces of retrieved context to answer the question. \n",
      "If you don't know the answer, just say that you don't know. \n",
      "Use three sentences maximum and keep the answer concise.\n",
      "Question: What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them? \n",
      "Context: [Document(page_content='Autoencoders are one of the deep learning types used for unsupervised learning. There are key layers of autoencoders, which are the input layer, encoder, bottleneck hidden layer, decoder, and output.', metadata={'source': 'textbook.md'}), Document(page_content=\"The three layers of the autoencoder are:-\\n1) Encoder - Compresses the input data to an encoded representation which is typically much smaller than the input data.\\n2) Latent Space Representation/ Bottleneck/ Code - Compact summary of the input containing the most important features\\n3) Decoder - Decompresses the knowledge representation and reconstructs the data back from its encoded form.\\nThen a loss function is used at the top to compare the input and output images.\\nNOTE- It's a requirement that the dimensionality of the input and output be the same. Everything in the middle can be played with.\", metadata={'source': 'textbook.md'}), Document(page_content='Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\\n\\n1. Transformers and Big Bird (Autoencoders is one of these components in both algorithms): Text Summarizer, Text Generator\\n2. Image compression\\n3. Nonlinear version of PCA\\n\\n---', metadata={'source': 'textbook.md'}), Document(page_content='4. Dropout rate: percent of nodes to drop temporarily during the forward pass.\\n\\n5. Kernel: matrix to perform dot product of image array with\\n\\n6. Activation function: defines how the weighted sum of inputs is transformed into outputs (e.g. tanh, sigmoid, softmax, Relu, etc)\\n\\n7. Number of epochs: number of passes an algorithm has to perform for training', metadata={'source': 'textbook.md'})] \n",
      "Answer:\n",
      "Autoencoders are deep learning models used for unsupervised learning, consisting of input, encoder, bottleneck hidden, decoder, and output layers. The encoder compresses input data into a compact summary, the bottleneck hidden layer represents the most important features, and the decoder reconstructs the data from its encoded form. Practical uses include text summarization and generation in algorithms like Transformers and Big Bird, image compression, and a nonlinear version of PCA. Other parameters that can be adjusted include dropout rate, kernel, activation function, and number of epochs for training.\n"
     ]
    }
   ],
   "source": [
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don\\'t know the answer, just say that you don\\'t know. \\nUse three sentences maximum and keep the answer concise.\\nQuestion: What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them? \\nContext: [Document(page_content=\\'Autoencoders are one of the deep learning types used for unsupervised learning. There are key layers of autoencoders, which are the input layer, encoder, bottleneck hidden layer, decoder, and output.\\', metadata={\\'source\\': \\'textbook.md\\'}), Document(page_content=\"The three layers of the autoencoder are:-\\\\n1) Encoder - Compresses the input data to an encoded representation which is typically much smaller than the input data.\\\\n2) Latent Space Representation/ Bottleneck/ Code - Compact summary of the input containing the most important features\\\\n3) Decoder - Decompresses the knowledge representation and reconstructs the data back from its encoded form.\\\\nThen a loss function is used at the top to compare the input and output images.\\\\nNOTE- It\\'s a requirement that the dimensionality of the input and output be the same. Everything in the middle can be played with.\", metadata={\\'source\\': \\'textbook.md\\'}), Document(page_content=\\'Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\\\\n\\\\n1. Transformers and Big Bird (Autoencoders is one of these components in both algorithms): Text Summarizer, Text Generator\\\\n2. Image compression\\\\n3. Nonlinear version of PCA\\\\n\\\\n---\\', metadata={\\'source\\': \\'textbook.md\\'}), Document(page_content=\\'4. Dropout rate: percent of nodes to drop temporarily during the forward pass.\\\\n\\\\n5. Kernel: matrix to perform dot product of image array with\\\\n\\\\n6. Activation function: defines how the weighted sum of inputs is transformed into outputs (e.g. tanh, sigmoid, softmax, Relu, etc)\\\\n\\\\n7. Number of epochs: number of passes an algorithm has to perform for training\\', metadata={\\'source\\': \\'textbook.md\\'})] \\nAnswer:\\nAutoencoders are deep learning models used for unsupervised learning, consisting of input, encoder, bottleneck hidden, decoder, and output layers. The encoder compresses input data into a compact summary, the bottleneck hidden layer represents the most important features, and the decoder reconstructs the data from its encoded form. Practical uses include text summarization and generation in algorithms like Transformers and Big Bird, image compression, and a nonlinear version of PCA. Other parameters that can be adjusted include dropout rate, kernel, activation function, and number of epochs for training.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
