{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('textbook.md')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 601, which is longer than the specified 500\n",
      "Created a chunk of size 1017, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n",
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/pranu/.cache/weaviate-embedded: process ID 4561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_e9282d99a6e34d2ba5d1aa652a811650_vMLiokhAG4dG in 1.382291ms\",\"time\":\"2024-03-25T15:45:37-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:37-07:00\",\"took\":43666}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_b8000fd8b17743eba95548d5160368f7_mRJfr05Iemk1 in 1.905666ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":53125}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_44bc55537e494c5caed789d7f3fc4add_fPABb6HlKwFJ in 5.682666ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_edfb3e05c45248f38cf1f2655353be2d_6a79MllkXFbg in 5.278166ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":259542}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":320125}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_bc386ce9499d466ca2e9b864bb454c7c_29Vu4XQHB23o in 6.566709ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_64813f7e10eb4c69ac244e1052bd0a34_pY7wJtNnAefU in 6.657417ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":104750}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_f0eb7e9550bb417bbc51caa6cef71c48_Lbk6OeygnrYx in 6.850375ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_e4124fc5f4ac4cd9b0d6ea2bc0d17416_KU3ejL4kkH1k in 6.853667ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":276750}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_a54fa1c47ff34c4f91febfe4cbd636c8_TgCujXtPh8QS in 6.794875ms\",\"time\":\"2024-03-25T15:45:38-07:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":265667}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":337333}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-25T15:45:38-07:00\",\"took\":248583}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "  client=client,\n",
    "  documents=chunks,\n",
    "  embedding=embeddings_model,\n",
    "  by_text=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "with open(\"questions.txt\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    answer = chain.invoke(question)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_answers = []\n",
    "\n",
    "for answer in answers:\n",
    "    words = answer.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word == 'Answer:':\n",
    "            processed_answers.append(\" \".join(words[i+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\", \"w\") as f:\n",
    "    for answer in processed_answers:\n",
    "        f.write(answer)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answers = []\n",
    "\n",
    "with open('textbook.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "for t in text.split('---'):\n",
    "    real_answers.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_answers(key_answers, student_answers):\n",
    "    nquestions = len(key_answers)\n",
    "    exam_data = {}\n",
    "    for i in range(1, nquestions+1):\n",
    "        \n",
    "        exam_data[str(i)] = [key_answers[i-1], [[student_answers[i-1]]]]\n",
    "    return exam_data\n",
    "\n",
    "exam_data = format_answers(processed_answers, real_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cannyeval import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing Existing Abbreviations\n",
      "Accumalating Abbreviations\n",
      "Total contents added :  0\n",
      "Total contents of ABBR_VOCAB :  11\n",
      "{'ANN': 'artificial neural network', 'SVM': 'support vector machines', 'NN': 'neural network', 'MLP': 'multi layer perceptron', 'FF': 'feed forward', 'RBFN': 'radial basis function network', 'MSE': 'mean squared error', 'RMSE': 'root mean squared error', 'MLE': 'maximum likelihood estimation', 'MAP': 'maximum aposteriori', 'MAE': 'mean absolute error'}\n",
      "Total contents added :  4\n",
      "Total contents of ABBR_VOCAB :  15\n",
      "{'ANN': 'artificial neural network', 'SVM': 'support vector machines', 'NN': 'neural network', 'MLP': 'multi layer perceptron', 'FF': 'feed forward', 'RBFN': 'radial basis function network', 'MSE': 'mean squared error', 'RMSE': 'root mean squared error', 'MLE': 'maximum likelihood estimation', 'MAP': 'maximum aposteriori', 'MAE': 'mean absolute error', 'PCA': 'principle component analysis', 'GRU': 'gated recurrent unit', 'LSTM': 'long short term memory', 'CNN': 'convolutional neural network'}\n",
      "New Abbreviation Vocab LU detected\n",
      "New Abbreviation Vocab NOTE detected\n",
      "New Abbreviation Vocab BN detected\n",
      "New Abbreviation Vocab RMSP detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Student1 against Question1 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 16.55it/s]\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Evaluating Student1 against Question2 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 15.24it/s]\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Evaluating Student1 against Question3 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 11.25it/s]\n",
      "Evaluating Student1 against Question4 ...: 100%|██████████████████████| 2/2 [00:00<00:00,  7.18it/s]\n",
      "Evaluating Student1 against Question5 ...: 100%|██████████████████████| 2/2 [00:00<00:00,  9.53it/s]\n",
      "Evaluating Student1 against Question6 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 16.96it/s]\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Evaluating Student1 against Question7 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 19.23it/s]\n",
      "Evaluating Student1 against Question8 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 21.29it/s]\n",
      "Evaluating Student1 against Question9 ...: 100%|██████████████████████| 2/2 [00:00<00:00, 27.64it/s]\n",
      "Evaluating Student1 against Question10 ...: 100%|█████████████████████| 2/2 [00:00<00:00, 22.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Evaluations Done\n",
      "            Question 1  Question 2  Question 3  Question 4  Question 5  Question 6  Question 7  Question 8  Question 9  Question 10  Student Aggregate\n",
      "0                 6.74        7.07        4.83        8.96         4.7        4.59        5.21         6.6         4.2         2.84              55.74\n",
      "Class Mean        6.74        7.07        4.83        8.96         4.7        4.59        5.21         6.6         4.2         2.84              55.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatorA = CannyEval(abbreviations_path='abbreviation_vocab.txt', flush_existing_abbr=True)\n",
    "evaluatorA.append_abbreviation_vocab({'PCA' : 'principle component analysis', 'GRU' : 'gated recurrent unit', 'LSTM' : 'long short term memory', 'CNN' : 'convolutional neural network'})\n",
    "\n",
    "report = evaluatorA.report_card(data_json=exam_data, max_marks=10, relative_marking=False, integer_marking=False, json_load_version=\"v2\")\n",
    "print(report.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(real_answers)):\n",
    "    real_embedding = embeddings_model.embed_query(real_answers[i])\n",
    "    predicted_embedding = embeddings_model.embed_query(processed_answers[i])\n",
    "    similarity = cosine_similarity([predicted_embedding], [real_embedding])[0][0]\n",
    "    similarities.append(similarity)\n",
    "\n",
    "similarities = np.asarray(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.5013488210679051\n",
      "Max similarity: 0.7761481078046336\n",
      "Min similarity: 0.036656902647697234\n",
      "Standard deviation of similarities: 0.208374607473384\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity:', np.mean(similarities))\n",
    "print('Max similarity:', np.max(similarities))\n",
    "print('Min similarity:', np.min(similarities))\n",
    "print('Standard deviation of similarities:', np.std(similarities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
