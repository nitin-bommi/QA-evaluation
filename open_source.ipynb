{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin/Documents/USC/Courses/CSCI 544/QA-evaluation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('ggrn/e5-small-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('textbook.txt', 'r') as f:\n",
    "    textbook = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "\n",
    "chunks = [textbook[i:i+chunk_size] for i in range(0, len(textbook), chunk_size)]\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to vectorize using E5-small-V2: 3.31s\n"
     ]
    }
   ],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = model.encode(input, normalize_embeddings=True)\n",
    "    return embeddings_batch_response\n",
    "\n",
    "start = time.time()\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken to vectorize using E5-small-V2: {round(end-start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to vectorize using BGE-small: 3.57s\n"
     ]
    }
   ],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = model.encode(input, normalize_embeddings=True)\n",
    "    return embeddings_batch_response\n",
    "\n",
    "start = time.time()\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken to vectorize using BGE-small: {round(end-start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "with open(\"questions.txt\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatAgent = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering question: 1\n",
      "Answering question: 2\n",
      "Answering question: 3\n",
      "Answering question: 4\n",
      "Answering question: 5\n",
      "Answering question: 6\n",
      "Answering question: 7\n",
      "Answering question: 8\n",
      "Time taken to answer questions on E5-small-V2: 964.73s\n"
     ]
    }
   ],
   "source": [
    "predicted_answers = []\n",
    "\n",
    "start = time.time()\n",
    "for i, question in enumerate(questions):\n",
    "    print(f'Answering question: {i + 1}')\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {retrieved_chunk}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    output = chatAgent(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=30, top_p=0.95)\n",
    "    output = output[0]['generated_text']\n",
    "    predicted_answers.append(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to answer questions on E5-small-V2: {round(end-start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering question: 1\n",
      "Answering question: 2\n",
      "Answering question: 3\n",
      "Answering question: 4\n",
      "Answering question: 5\n",
      "Answering question: 6\n",
      "Answering question: 7\n",
      "Answering question: 8\n",
      "Time taken to answer questions on BGE-small: 1086.67s\n"
     ]
    }
   ],
   "source": [
    "predicted_answers = []\n",
    "\n",
    "start = time.time()\n",
    "for i, question in enumerate(questions):\n",
    "    print(f'Answering question: {i + 1}')\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {retrieved_chunk}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    output = chatAgent(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=30, top_p=0.95)\n",
    "    output = output[0]['generated_text']\n",
    "    predicted_answers.append(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to answer questions on BGE-small: {round(end-start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answers = []\n",
    "\n",
    "with open('actual.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "for t in text.split('---'):\n",
    "    real_answers.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers_processed = []\n",
    "\n",
    "for answer in predicted_answers:\n",
    "    for i, word in enumerate(answer.split()):\n",
    "        if word == 'Answer:':\n",
    "            predicted_answers_processed.append(' '.join(answer.split()[i+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answers_embeddings = np.array([get_text_embedding(answer) for answer in real_answers])\n",
    "predicted_answers_embeddings = np.array([get_text_embedding(answer) for answer in predicted_answers_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = []\n",
    "euclidean_similarities = []\n",
    "\n",
    "for i in range(len(real_answers)):\n",
    "    cosine_sim = cosine_similarity([real_answers_embeddings[i]], [predicted_answers_embeddings[i]])[0][0]\n",
    "    cosine_similarities.append(cosine_sim)\n",
    "\n",
    "cosine_similarities = np.array(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted.txt', 'w') as f:\n",
    "    for answer in predicted_answers_processed:\n",
    "        f.write(f'{answer}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84720886"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
